# -*- coding: utf-8 -*-
"""121 Glory ML Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IHgwTlw-10XzN3IjJhtYbzi-oF7cyUWW
"""

# Part 1: Data Preprocessing & Exploration
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Load dataset
df = pd.read_csv('survey lung cancer.csv')

# Display first few rows
print(df.head())

# Handle missing values (fill numeric columns with mean)
df.fillna(df.select_dtypes(include=['number']).mean(), inplace=True)

#removing duplicate values
df= df.drop_duplicates()

# Convert categorical values
df.replace({'YES': 1, 'NO': 0, 'M': 1, 'F': 0}, inplace=True)

# Encoding categorical variables
label_enc = LabelEncoder()
if 'GENDER' in df.columns:
    df['GENDER'] = label_enc.fit_transform(df['GENDER'])


# Data distributions
sns.pairplot(df, diag_kind='kde')
plt.show()

# Correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.show()

# Outlier detection
sns.boxplot(data=df)
plt.show()

# Summary statistics
summary_stats= df[['AGE', 'SMOKING', 'ANXIETY']].describe()
print(summary_stats)

# Feature engineering
df['risk_category'] = pd.cut(df['SMOKING'], bins=[0, 5, 10, 15], labels=['Low', 'Medium', 'High'])

# Part 2: Feature Engineering & Selection

# Select relevant features
selected_features = ["AGE", "GENDER", "SMOKING", "CHRONIC DISEASE"]
X = df[selected_features]
y = df["LUNG_CANCER"]

# Standardize numerical features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Save the scaler
import pickle
pickle.dump(scaler, open("scaler.pkl", "wb"))

import matplotlib.pyplot as plt
import seaborn as sns

# Histograms for all columns
df.hist(figsize=(15, 10), bins=20, color='skyblue', edgecolor='black')
plt.suptitle('Distribution of All Features')
plt.show()

# Box plots for all columns
plt.figure(figsize=(15, 10))
for column in df.columns:
    sns.boxplot(x=df[column])
    plt.title(f'Box Plot - {column}')
    plt.show()
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
cmap = sns.diverging_palette(220, 10, as_cmap=True)
sns.heatmap(df.corr())
plt.title('Correlation Heatmap')
plt.show()

# Removing outliers using Z-score
from scipy.stats import zscore

z_scores = zscore(df)
abs_z_scores = abs(z_scores)
filtered_entries = (abs_z_scores < 3).all(axis=1)
df = df[filtered_entries]

# Part 3: Model Development & Training
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train models
lr_model = LogisticRegression()
lr_model.fit(X_train, y_train)

dt_model = DecisionTreeClassifier()
dt_model.fit(X_train, y_train)

# Save models
pickle.dump(lr_model, open("logistic_regression.pkl", "wb"))
pickle.dump(dt_model, open("decision_tree.pkl", "wb"))

# Model Evaluation
lr_preds = np.round(lr_model.predict(X_test))  # Rounding for classification
dt_preds = dt_model.predict(X_test)

print("Logistic Regression Model")
print(classification_report(y_test, lr_preds))
print("Logistic Regression Accuracy",accuracy_score(y_test, lr_preds))
print("Decision Tree Model")
print(classification_report(y_test, dt_preds))
print("Decision Tree Model Accuracy",accuracy_score(y_test, dt_preds))

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

# Logistic Regression
lr_model = LogisticRegression(random_state=0)
lr_model.fit(X_train, y_train)
logreg_pred = lr_model.predict(X_test)

# Decision Tree
dt_model = DecisionTreeClassifier(criterion='entropy', random_state=0)
dt_model.fit(X_train, y_train)
decision_tree_pred = dt_model.predict(X_test)

# Evaluate the models
logreg_accuracy = accuracy_score(y_test, logreg_pred)
decision_tree_accuracy = accuracy_score(y_test, decision_tree_pred)

#predict accuracy
print("Logistic Regression Accuracy:", logreg_accuracy)
print("Decision Tree Accuracy:", decision_tree_accuracy)

from sklearn.metrics import confusion_matrix as cm

# Confusion Matrix
conf_matrix = cm(y_test, logreg_pred)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix - Logistic Regression')
plt.show()

# Confusion Matrix for Decision Tree
conf_matrix_dt = confusion_matrix(y_test, decision_tree_pred)
plt.figure(figsize=(5, 5))
sns.heatmap(conf_matrix_dt, annot=True, cmap='Blues', fmt='d')
plt.title("Decision Tree Confusion Matrix")
plt.show()

# Save models
pickle.dump(lr_model, open("linear_regression.pkl", "wb"))
pickle.dump(dt_model, open("decision_tree.pkl", "wb"))
print("Models saved successfully!")

# Part 5: Model Deployment with Flask
from flask import Flask, request, jsonify, render_template
import os

app = Flask(__name__)

# Load models and scaler
if os.path.exists("scaler.pkl") and os.path.exists("decision_tree.pkl"):
    scaler = pickle.load(open("scaler.pkl", "rb"))
    dt_model = pickle.load(open("decision_tree.pkl", "rb"))

@app.route('/')
def home():
    return render_template('form.html')

@app.route('/predict', methods=['POST'])
def predict():
    try:
        features = [float(request.form.get(key)) for key in ["AGE", "GENDER", "SMOKING", "CHRONIC DISEASE"]]
        input_data = np.array(features).reshape(1, -1)
        input_data = scaler.transform(input_data)
        prediction = dt_model.predict(input_data)
        return render_template('form.html', prediction_text=f'Predicted Disease Status: {prediction[0]}')
    except Exception as e:
        return render_template('form.html', prediction_text=f'Error: {str(e)}')

if __name__ == "__main__":
    app.run(debug=True,port=8000)